# FeedbackHub on EKS

A production-ready, cloud-native deployment of the FeedbackHub application on Amazon EKS with comprehensive monitoring, logging, and security capabilities.

## üöÄ Overview

This project migrates the FeedbackHub application from AWS ECS to Amazon EKS, providing enhanced scalability, observability, and cloud-native features. The infrastructure is built using modular Terraform configurations with best practices for security, monitoring, and cost optimization.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              FeedbackHub EKS Architecture                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                Application Layer                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Next.js Frontend  ‚îÇ    ‚îÇ     Next.js API     ‚îÇ    ‚îÇ    MongoDB Atlas    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   (React/TypeScript)‚îÇ    ‚îÇ   (Node.js/Express) ‚îÇ    ‚îÇ   (Managed Service) ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ    ‚îÇ                     ‚îÇ    ‚îÇ                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Responsive UI     ‚îÇ    ‚îÇ ‚úì REST API          ‚îÇ    ‚îÇ ‚úì Cloud Database    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì User Management   ‚îÇ    ‚îÇ ‚úì Authentication    ‚îÇ    ‚îÇ ‚úì High Availability ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Feedback Forms    ‚îÇ    ‚îÇ ‚úì Data Validation   ‚îÇ    ‚îÇ ‚úì Automated Backups ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                              Kubernetes Platform                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ     Monitoring      ‚îÇ    ‚îÇ       Logging       ‚îÇ    ‚îÇ      Ingress        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ    ‚îÇ                     ‚îÇ    ‚îÇ                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Prometheus        ‚îÇ    ‚îÇ ‚úì Loki              ‚îÇ    ‚îÇ ‚úì ALB Controller    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Grafana           ‚îÇ    ‚îÇ ‚úì Promtail          ‚îÇ    ‚îÇ ‚úì TLS Termination   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Alertmanager      ‚îÇ    ‚îÇ ‚úì CloudWatch        ‚îÇ    ‚îÇ ‚úì Path-based Routing‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Node Exporter     ‚îÇ    ‚îÇ ‚úì Fluent Bit        ‚îÇ    ‚îÇ ‚úì Health Checks     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                EKS Cluster                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                            Control Plane (Managed)                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Managed Node      ‚îÇ    ‚îÇ   Spot Instance     ‚îÇ    ‚îÇ      Fargate        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      Groups         ‚îÇ    ‚îÇ    Node Groups      ‚îÇ    ‚îÇ    (Optional)       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ    ‚îÇ                     ‚îÇ    ‚îÇ                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Auto Scaling      ‚îÇ    ‚îÇ ‚úì Cost Optimization ‚îÇ    ‚îÇ ‚úì Serverless        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Security Patches  ‚îÇ    ‚îÇ ‚úì Mixed Workloads   ‚îÇ    ‚îÇ ‚úì Zero Management   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚úì Instance Refresh  ‚îÇ    ‚îÇ ‚úì Fault Tolerance   ‚îÇ    ‚îÇ ‚úì Isolated Compute  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                               Network Layer                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                                 VPC                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Public Subnets ‚îÇ    ‚îÇ Private Subnets ‚îÇ    ‚îÇ   Security & Access    ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                         ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚úì NAT Gateways  ‚îÇ    ‚îÇ ‚úì EKS Nodes     ‚îÇ    ‚îÇ ‚úì Security Groups       ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚úì Load Balancers‚îÇ    ‚îÇ ‚úì Application   ‚îÇ    ‚îÇ ‚úì Network ACLs          ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚úì Internet GW   ‚îÇ    ‚îÇ   Workloads     ‚îÇ    ‚îÇ ‚úì IAM Roles (IRSA)      ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚úì Bastion Host  ‚îÇ    ‚îÇ ‚úì Databases     ‚îÇ    ‚îÇ ‚úì VPC Flow Logs         ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Placeholder diagram: see `docs/screenshots/architecture.png`.

## ‚ú® Key Features

### üèóÔ∏è **Infrastructure as Code**
- **Modular Terraform** configuration with reusable modules
- **Multi-environment** support (dev, staging, prod)
- **State management** with S3 backend and DynamoDB locking
- **Automated deployments** with CI/CD integration

### üîê **Security & Compliance**
- **IAM Roles for Service Accounts (IRSA)** for secure AWS service access
- **Network segmentation** with private subnets for workloads
- **Encryption at rest** for EBS volumes and Kubernetes secrets
- **Security contexts** with non-root containers and read-only filesystems
- **Network policies** for pod-to-pod communication control
- **VPC Flow Logs** for network traffic analysis

### üìä **Observability & Monitoring**
- **Prometheus** for metrics collection and alerting
- **Grafana** with pre-built dashboards for Kubernetes and applications
- **Alertmanager** for intelligent alert routing and silencing
- **Loki** for centralized log aggregation and analysis
- **CloudWatch** integration for AWS service logs
- **Distributed tracing** ready with OpenTelemetry support

### üí∞ **Cost Optimization**
- **Spot instances** for non-critical workloads (up to 90% savings)
- **Cluster autoscaler** for dynamic node scaling
- **GP3 storage** with optimized IOPS and throughput
- **Resource quotas** and limits for efficient resource utilization
- **Development environment** optimizations

### üöÄ **High Availability & Scalability**
- **Multi-AZ deployment** for fault tolerance
- **Horizontal Pod Autoscaler** for application scaling
- **Vertical Pod Autoscaler** for right-sizing (optional)
- **Persistent storage** with automatic backup and recovery
- **Rolling updates** with zero-downtime deployments

## üìÅ Project Structure

```
feedbackhub-on-eks/
‚îú‚îÄ‚îÄ üìù .github/
‚îÇ   ‚îî‚îÄ‚îÄ copilot-instructions.md    # GitHub Copilot workflow instructions
‚îú‚îÄ‚îÄ üöÄ app/                        # Next.js application source code
‚îÇ   ‚îú‚îÄ‚îÄ components/                # React components
‚îÇ   ‚îú‚îÄ‚îÄ pages/                     # Next.js pages and API routes
‚îÇ   ‚îú‚îÄ‚îÄ lib/                       # Utility libraries and database
‚îÇ   ‚îú‚îÄ‚îÄ public/                    # Static assets
‚îÇ   ‚îú‚îÄ‚îÄ styles/                    # CSS and styling
‚îÇ   ‚îú‚îÄ‚îÄ package.json               # Node.js dependencies
‚îÇ   ‚îú‚îÄ‚îÄ next.config.js             # Next.js configuration
‚îÇ   ‚îî‚îÄ‚îÄ tsconfig.json              # TypeScript configuration
‚îú‚îÄ‚îÄ üê≥ docker/                     # Docker configurations
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Production container image
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.dev             # Development container image
‚îÇ   ‚îî‚îÄ‚îÄ .dockerignore              # Docker ignore patterns
‚îú‚îÄ‚îÄ ‚ò∏Ô∏è k8s/                        # Kubernetes manifests
‚îÇ   ‚îî‚îÄ‚îÄ manifests/                 # Application deployment configs
‚îÇ       ‚îú‚îÄ‚îÄ namespace.yaml         # Application namespace
‚îÇ       ‚îú‚îÄ‚îÄ configmap.yaml         # Configuration data
‚îÇ       ‚îú‚îÄ‚îÄ secret.yaml            # Sensitive configuration
‚îÇ       ‚îú‚îÄ‚îÄ deployment.yaml        # Application deployment
‚îÇ       ‚îú‚îÄ‚îÄ service.yaml           # Service definition
‚îÇ       ‚îú‚îÄ‚îÄ ingress.yaml           # Ingress configuration
‚îÇ       ‚îî‚îÄ‚îÄ hpa.yaml               # Horizontal Pod Autoscaler
‚îú‚îÄ‚îÄ üõ†Ô∏è scripts/                    # Build and deployment scripts
‚îÇ   ‚îú‚îÄ‚îÄ build.sh                   # Application build script
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                  # Deployment automation
‚îÇ   ‚îú‚îÄ‚îÄ health-check.sh            # Health verification
‚îÇ   ‚îî‚îÄ‚îÄ cleanup.sh                 # Resource cleanup
‚îú‚îÄ‚îÄ üèóÔ∏è terraform/                  # Infrastructure modules
‚îÇ   ‚îú‚îÄ‚îÄ network/                   # VPC, subnets, security groups
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ cluster/                   # EKS cluster configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ nodegroups/               # Managed node groups
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ irsa/                     # IAM roles for service accounts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ alb-controller/           # AWS Load Balancer Controller
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/               # Prometheus, Grafana, Alertmanager
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ logging/                  # Loki, Promtail, CloudWatch
‚îÇ       ‚îú‚îÄ‚îÄ main.tf
‚îÇ       ‚îú‚îÄ‚îÄ variables.tf
‚îÇ       ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ üéØ infra/                     # Root infrastructure orchestration
    ‚îú‚îÄ‚îÄ main.tf                   # Main Terraform configuration
    ‚îú‚îÄ‚îÄ variables.tf              # Variable definitions
    ‚îú‚îÄ‚îÄ outputs.tf                # Output values
    ‚îú‚îÄ‚îÄ backend-dev.tf            # Local backend (safe for dev)
    ‚îú‚îÄ‚îÄ backend-prod.tf           # Commented S3 backend (for later)
    ‚îú‚îÄ‚îÄ dev.tfvars                # Development defaults (no NAT/ALB)
    ‚îú‚îÄ‚îÄ prod.tfvars               # Production defaults
    ‚îî‚îÄ‚îÄ README.md                 # Infrastructure documentation
```

## üöÄ Quick Start

### Prerequisites

Ensure you have the following tools installed:

- [AWS CLI](https://aws.amazon.com/cli/) v2.x configured with appropriate permissions
- [Terraform](https://www.terraform.io/) >= 1.0
- [kubectl](https://kubernetes.io/docs/tasks/tools/) for Kubernetes management
- [Helm](https://helm.sh/) >= 3.0 for package management
- [Docker](https://www.docker.com/) for local development and building

### 1. Run in dev mode (cost-aware, no apply)

```bash
# Clone the repository
git clone <your-repo-url>
cd feedbackhub-on-eks/infra

# Initialize and validate
terraform init
terraform validate

# Preview dev changes (no-cost defaults: NAT/ALB/Ingress disabled)
terraform plan -var-file=dev.tfvars
```

### 2. Verify Deployment

```bash
# Check cluster status
kubectl get nodes
kubectl get pods --all-namespaces

# Check monitoring stack
kubectl get pods -n monitoring

# Check logging stack
kubectl get pods -n logging
```

### 3. Access Applications

```bash
# Access Grafana (monitoring)
kubectl port-forward -n monitoring svc/grafana 3000:80
# Visit http://localhost:3000 (admin/your-password)

# Access Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# Visit http://localhost:9090

# Access Loki (logging)
kubectl port-forward -n logging svc/loki 3100:3100
# Visit http://localhost:3100
```

### 4. Deploy Application

```bash
# Deploy the FeedbackHub application
cd ../k8s
kubectl apply -f manifests/

# Check application status
kubectl get pods -n feedbackhub
kubectl get ingress -n feedbackhub
```

## üîß Configuration

### Environment Variables

The infrastructure supports multiple environments through Terraform variable files:

- **Production**: `terraform.tfvars` (full-featured, high availability)
- **Development**: `terraform.tfvars.dev` (cost-optimized, minimal resources)
- **Staging**: Create `terraform.tfvars.staging` for testing

### Key Configuration Options

| Category | Variable | Description | Default |
|----------|----------|-------------|---------|
| **General** | `aws_region` | AWS region for deployment | `us-east-1` |
| | `environment` | Environment name | `prod` |
| **Cluster** | `cluster_version` | EKS version | `1.29` |
| | `node_group_instance_types` | EC2 instance types | `["t3.medium"]` |
| **Cost** | `enable_spot_instances` | Enable spot instances | `false` |
| | `single_nat_gateway` | Use single NAT gateway | `false` |
| **Monitoring** | `enable_monitoring` | Enable Prometheus/Grafana | `true` |
| | `prometheus_storage_size` | Prometheus storage | `50Gi` |
| **Logging** | `enable_logging` | Enable Loki/Promtail | `true` |
| | `log_collector` | Log collector type | `promtail` |

## üìä Monitoring & Observability

### Grafana Dashboards

The monitoring stack includes pre-configured dashboards:

1. **Kubernetes Cluster Overview** - Cluster-wide metrics and health
2. **Node Exporter Full** - Detailed node-level metrics
3. **Pod Monitoring** - Per-pod resource usage and performance
4. **Application Performance** - Custom application metrics
5. **Cost Monitoring** - Resource usage and cost optimization

### Prometheus Metrics

Key metrics collected:

- **Infrastructure**: CPU, memory, disk, network
- **Kubernetes**: Pod status, resource usage, events
- **Application**: Custom business metrics, request rates, error rates
- **Cost**: Resource consumption, spot instance savings

### Log Analysis

**Loki** provides centralized logging with:

- **Structured logs** from all applications
- **Kubernetes events** and pod logs
- **System logs** from nodes
- **AWS service logs** via CloudWatch

### Alerting

**Alertmanager** provides intelligent alerting:

- **Critical alerts**: Node failures, pod crashes
- **Warning alerts**: High resource usage, slow response times
- **Custom alerts**: Business logic failures, SLA violations

## üîí Security

### Defense in Depth

1. **Network Security**
   - Private subnets for all workloads
   - Security groups with minimal required access
   - Network policies for pod-to-pod communication
   - VPC Flow Logs for traffic analysis

2. **Identity & Access Management**
   - IAM Roles for Service Accounts (IRSA)
   - Least privilege access principles
   - Regular access reviews and rotation

3. **Container Security**
   - Non-root containers
   - Read-only root filesystems
   - Security contexts and capabilities
   - Image vulnerability scanning

4. **Data Protection**
   - Encryption at rest for all storage
   - TLS encryption in transit
   - Secrets management with Kubernetes secrets
   - Regular backup and recovery testing

### Compliance

The infrastructure is designed to meet common compliance requirements:

- **SOC 2** - Security monitoring and logging
- **PCI DSS** - Network segmentation and access controls
- **GDPR** - Data encryption and audit trails
- **HIPAA** - Encryption and access logging (with additional configs)

## üí∞ Cost Optimization

### Development Environment Savings

The development configuration provides significant cost savings:

| Resource | Production | Development | Savings |
|----------|------------|-------------|---------|
| **Instances** | t3.medium | t3.small | ~50% |
| **NAT Gateway** | Multi-AZ | Single | ~67% |
| **Storage** | 50Gi | 10Gi | ~80% |
| **Monitoring** | Full stack | Minimal | ~60% |
| **Logging** | CloudWatch + Loki | Loki only | ~40% |

### Production Optimizations

- **Spot instances** for batch workloads (90% savings)
- **GP3 storage** with optimized IOPS
- **Resource quotas** to prevent over-provisioning
- **Cluster autoscaler** for dynamic scaling
- **Right-sizing** with Vertical Pod Autoscaler

### Cost Monitoring

Set up cost monitoring and alerts:

```bash
# Enable cost allocation tags
aws ce put-dimension-key --dimension-key Project
aws ce put-dimension-key --dimension-key Environment

# Create cost budget
aws budgets create-budget \
  --account-id 123456789012 \
  --budget file://budget.json \
  --notifications-with-subscribers file://notifications.json
```

## üîÑ CI/CD Integration

### GitHub Actions Workflow

```yaml
name: Deploy FeedbackHub
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-node@v4
      with:
        node-version: '18'
    - run: npm ci
      working-directory: ./app
    - run: npm test
      working-directory: ./app

  infrastructure:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    - uses: hashicorp/setup-terraform@v3
    - name: Deploy Infrastructure
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        cd infra
        terraform init
        terraform plan
        terraform apply -auto-approve

  deploy:
    needs: infrastructure
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Deploy Application
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        aws eks update-kubeconfig --name feedbackhub-prod
        kubectl apply -f k8s/manifests/
```

### GitLab CI/CD

```yaml
stages:
  - test
  - infrastructure
  - deploy

test:
  stage: test
  image: node:18
  script:
    - cd app
    - npm ci
    - npm test

infrastructure:
  stage: infrastructure
  image: hashicorp/terraform:latest
  script:
    - cd infra
    - terraform init
    - terraform plan
    - terraform apply -auto-approve
  only:
    - main

deploy:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - aws eks update-kubeconfig --name feedbackhub-prod
    - kubectl apply -f k8s/manifests/
  only:
    - main
```

## üÜò Troubleshooting

### Common Issues

#### 1. **Cluster Access Issues**

```bash
# Verify AWS credentials
aws sts get-caller-identity --no-cli-pager

# Update kubeconfig
aws eks --no-cli-pager --region us-east-1 update-kubeconfig --name feedbackhub-prod

# Check cluster status
kubectl cluster-info
```

#### 2. **Pod Scheduling Issues**

```bash
# Check node capacity
kubectl describe nodes
kubectl top nodes

# Check resource requests
kubectl describe pod <pod-name>

# Check node taints and tolerations
kubectl get nodes -o yaml | grep -A5 -B5 taints
```

#### 3. **Monitoring Not Working**

```bash
# Check Prometheus targets
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# Visit http://localhost:9090/targets

# Check Grafana datasources
kubectl logs -n monitoring deployment/grafana

# Verify service monitors
kubectl get servicemonitor -n monitoring
```

#### 4. **Application Load Balancer Issues**

```bash
# Check ALB controller logs
kubectl logs -n kube-system deployment/aws-load-balancer-controller

# Verify IRSA configuration
kubectl describe serviceaccount aws-load-balancer-controller -n kube-system

# Check ingress status
kubectl describe ingress <ingress-name>
```

### Health Check Commands

```bash
# Infrastructure health
terraform plan  # Check for drift
kubectl get componentstatuses
kubectl get nodes
kubectl get pods --all-namespaces | grep -v Running

# Application health
kubectl get pods -n feedbackhub
kubectl logs -f deployment/feedbackhub-api -n feedbackhub

# Monitoring health
curl -s http://localhost:9090/-/healthy  # Prometheus
curl -s http://localhost:3000/api/health  # Grafana

# Logging health
curl -s http://localhost:3100/ready  # Loki
```

## üìà Performance Tuning

### Application Performance

1. **Resource Optimization**
   ```yaml
   resources:
     requests:
       memory: "256Mi"
       cpu: "250m"
     limits:
       memory: "512Mi"
       cpu: "500m"
   ```

2. **Horizontal Pod Autoscaler**
   ```yaml
   apiVersion: autoscaling/v2
   kind: HorizontalPodAutoscaler
   metadata:
     name: feedbackhub-api-hpa
   spec:
     scaleTargetRef:
       apiVersion: apps/v1
       kind: Deployment
       name: feedbackhub-api
     minReplicas: 2
     maxReplicas: 10
     metrics:
     - type: Resource
       resource:
         name: cpu
         target:
           type: Utilization
           averageUtilization: 70
   ```

### Cluster Performance

1. **Node Group Optimization**
   - Use latest generation instances (t3, m5, r5)
   - Enable GP3 storage with optimized IOPS
   - Configure cluster autoscaler for dynamic scaling

2. **Network Performance**
   - Use enhanced networking (SR-IOV)
   - Enable container insights for network monitoring
   - Optimize service mesh configuration

## üîÑ Backup & Recovery

### Infrastructure Backup

```bash
# Export Terraform state
terraform show > terraform-state-backup.json

# Backup cluster configuration
kubectl get all --all-namespaces -o yaml > cluster-backup.yaml

# Export secrets (encrypted)
kubectl get secrets --all-namespaces -o yaml > secrets-backup.yaml.enc
```

### Application Data Backup

```bash
# Volume snapshots
kubectl create volumesnapshot prometheus-backup \
  --namespace monitoring \
  --source-pvc prometheus-storage

# MongoDB Atlas automated backups (configured in Atlas)
# Point-in-time recovery available
```

### Disaster Recovery

1. **Infrastructure Recovery**
   ```bash
   # Redeploy infrastructure
   terraform apply
   
   # Restore cluster configuration
   kubectl apply -f cluster-backup.yaml
   ```

2. **Data Recovery**
   ```bash
   # Restore from volume snapshots
   kubectl create pvc prometheus-storage-restored \
     --from-snapshot prometheus-backup
   
   # MongoDB restore (Atlas)
   # Use Atlas point-in-time recovery or cluster restore
   ```

## üìö Additional Resources

### Documentation
- [AWS EKS Best Practices](https://docs.aws.amazon.com/eks/latest/best-practices/)
- [Kubernetes Documentation](https://kubernetes.io/docs/home/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Prometheus Monitoring](https://prometheus.io/docs/introduction/overview/)
- [Grafana Documentation](https://grafana.com/docs/)

### Learning Resources
- [EKS Workshop](https://www.eksworkshop.com/)
- [Kubernetes the Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)
- [Cloud Native Computing Foundation](https://www.cncf.io/)

### Community
- [AWS Containers Roadmap](https://github.com/aws/containers-roadmap)
- [Kubernetes Slack](https://slack.k8s.io/)
- [CNCF Slack](https://slack.cncf.io/)

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôã‚Äç‚ôÇÔ∏è Support

For support and questions:

1. üìñ Check the documentation in each module's README
2. üêõ Search existing [issues](https://github.com/your-org/feedbackhub-on-eks/issues)
3. üí¨ Join our [Slack community](https://your-slack-invite-link)
4. ‚úâÔ∏è Contact the DevOps team

---

**üéØ Built with ‚ù§Ô∏è for modern cloud-native applications**

*Empowering developers with production-ready infrastructure*
